---
title: "Marketing Campaign for a Housing Subsidy"
author: "Ben Keel"
date: "2022-11-04"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

# Who Would Take a Subsidy?

Home repair subsidy programs can be a great return on investment across cities and states. In addition to tax benefits, programs like Pennsylvania's recent bill [Whole-Home Repairs Program]() stymie blight, vacancy, and other condition problems that can negatively affect communities in compounding ways. The idea behind Pennsylvania's bill was to tackle several issues at once, including the lack of housing that people can afford, unsafe homes that push out residents, the growing threat of damage to homes because of climate change, and inefficient buildings that waste energy and hike up residents' utility bills."

In this scenario, the fictional Emil City wants to take a more proactive approach to promoting their home repair subsidy program. Though the program has existed for 20 years prior, they usually reach out to eligible home owners at random, which has not provided a great response rate. The Department of Housing and Community Development (HCD) proposes a targeted marketing campaign. HCD collectively understands that the process of any good design relies of substantive, evidence-based feedback for the design to improve, and marketing campaigns are no exception. Fortunately, they have the data to help the design of their program be more efficient.

Our efforts in this analysis hope to refine a prediction model and forecast which citizens are most likely to participate in the program when marketed to. Then we'll use some assumed numbers to create a cost/benefit calculation, which will help the city know how conservative to be with their targeting. By the end, the model should provide a better direction for the city than the random chance, and can be fine-tuned toward improving the city's bottom line.

# Important Features

## Data Collecting

First step is to import the data source and libraries to be used for analysis.

```{r setup, warning = FALSE, message = FALSE}

library(tidyverse)
library(kableExtra)
library(caret)
library(knitr) 
library(pscl)
library(plotROC)
library(pROC)
library(lubridate)
library(broom)
library(stargazer)
library(ggplot2)
library(gridExtra)

knitr::opts_chunk$set(echo = TRUE)

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

options(scipen = '999')

palette5 <- c("#981FAC","#CB0F8B","#FF006A","#FE4C35","#FE9900")
palette4 <- c("#981FAC","#FF006A","#FE4C35","#FE9900")
palette2 <- c("#981FAC","#FF006A")

g <- glimpse

program <- read.csv("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter7/compas-scores-two-years.csv")
g(program)

subsidy$y <- factor(subsidy$y, levels = c("yes", "no"))

```

## Current Features

Below are collections of graphs to be used for feature selection when building a more effective model. Our data has results from previous campaigns that will enable a forecast of future participation in the housing subsidy. A "yes" here means that the person chose to participate, and may or may not have completed the application completely and received a subsidy.

```{r DV Continuous likelihood, warning = FALSE, message = FALSE}

subsidy %>%
  dplyr::select(y, age, spent_on_repairs, inflation_rate, unemploy_rate, previous, campaign) %>%
  gather(Variable, value, -y) %>%
    ggplot(aes(y, value, fill=y)) + 
      geom_bar(position = "dodge", stat = "summary", fun = "mean") + 
      facet_wrap(~Variable, scales = "free") +
      scale_fill_manual(values = palette2) +
      labs(x="Participated?", y="Value", 
           title = "Feature associations with the likelihood of Participation",
           subtitle = "(continous outcomes)") +
      theme(legend.position = "none")


```

Age appears to have little bearing, at least when displayed in this form, same with the amount that a household spent on repairs previously. More useful indicators seem to be The current unemployment rate and whether or not the household had been involved with a previous campaign.

However, plotting age across a continuous stream illustrates some different patterns of participation at different ranges of age. Similar with the "spent_on_repairs" vector, these features could be adjusted for more relevancy in the model. Other fields remain difficult to parse, like the consumer confidence and price indices.

```{r DV Continuous Distribution}

subsidy %>%
    dplyr::select(y, age, cons.conf.idx, cons.price.idx, unemploy_rate, inflation_rate, spent_on_repairs) %>%
    gather(Variable, value, -y) %>%
    ggplot() + 
    geom_density(aes(value, color=y), fill = "transparent") + 
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2) +
    labs(title = "Feature Distributions of Participation",
         subtitle = "Continous Outcomes",
         legend.title= "Result")


```

The bar charts below show categorical variables from the data set, both in the citizens's rates of participation and the raw counts for each category. When trying to find the right indicators of whether someone participated in the program or not, certain categories appear to be very relevant when their ratio of yes's to no's. However, double checking the raw counts reveal that some of the categories with the highest ratio of yes's to no's may not have a relatively sufficient number to compare to other categories. The job numbers, with its large variation between the raw counts of citizens marketed to, are a good example here. If their rate may warped by the low count of citizens in that cross-tab, then it serves as a less effective indicator than we may hope for, and we can ignore it when engineering features.

```{r DV Categorical Features: Personal Info, warning = FALSE, message = FALSE}


#Rate Graphs
subsidy %>%
    dplyr::select(y, job, marital, education, mortgage, taxbill_in_phl) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y) %>%
    group_by(Variable) %>%
      mutate(rate = lag(n)/n)%>%
    filter(y == "no" & value != "illiterate")%>%
      ggplot(., aes(value, rate)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        labs(x="Category", y="Ratio of 'Yes' to 'No'",
             title = "Feature Associations with the Likelihood of Participation",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))


#Quantity Graphs
subsidy %>%
    dplyr::select(y, job, marital, education, taxLien, mortgage, taxbill_in_phl) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y) %>%
      ggplot(., aes(value, n, fill = y)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        scale_fill_manual(values = palette2) +
        labs(x="Category", y="Value",
             title = "Feature Associations with Counts of Participation",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The same conclusion can be drawn from the "months" data illustrated below. December has a small amount of data recorded, yet it is up with March as one of the best times to contact someone to increase likelihood of participation. These differences are helpful when changing rates.

```{r DV Categorical Features: Campaign Contact Info, warning = FALSE, message = FALSE}

#Rate Graphs
subsidy %>%
    dplyr::select(y, contact, month, day_of_week, poutcome) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y)%>%
  mutate(rate = lag(n)/n)%>%
    filter(y=="no")%>%
      ggplot(., aes(value, rate)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        labs(x="Category", y="Ratio of 'Yes' to 'No'",
             title = "Feature associations with the likelihood of Participation",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Quantity Graphs
subsidy %>%
    dplyr::select(y, contact, month, day_of_week, poutcome) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y) %>%
      ggplot(., aes(value, n, fill = y)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        scale_fill_manual(values = palette2) +
        labs(x="Category", y="Value",
             title = "Feature Associations with Counts of Participation",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

## Data Split + Modeling

To start with the modeling, we split the base data set into 65% and 35% parts, then throw every variable into the model to get a baseline read on how the logistic regression will interpret this data.

```{r Data Partition and "Kitchen Sink" Regression, warning = FALSE, message = FALSE}

set.seed(999)
trainIndex <- createDataPartition(subsidy$y, p = .65,
                                  list = FALSE,
                                  times = 1)
subsidyTrain <- subsidy[ trainIndex,]
subsidyTest  <- subsidy[-trainIndex,]

subsidyModel <- glm(y_numeric ~ .,
                  data=subsidyTrain %>% 
                    dplyr::select(-y),
                  family="binomial" (link="logit"))

summary(subsidyModel)

```

Many of the features do not appear to be significant, signaling that we can do some cleaning of the features to avoid risking multi-collinearity, overlapping the effects that different features have on the prediction.

Looking at the McFadden R-Squared Value below, one would hope to move this value closer to 1 when engineering more relevant features.

```{r Standard Reg McFadden R-squared, warning = FALSE, message = FALSE}
regSubsidySummary <- 
  tidy(pR2(subsidyModel))%>%
  mutate(Regression = "Kitchen Sink")

regSubsidySummary%>%
  filter(names=="McFadden")%>%
  rename(Value = x,
         Measure = names)%>%
    kable(
    caption = "<strong></strong>",
    escape= FALSE,
    format="html",
    row.names = FALSE,
    align="l")%>%
  kable_styling()

```

## New Features

Four new features were added or attempted for this sake, to increase the predictive relevancy of logistic regression and account for variance with tangible terms. First, the "spent_on_repairs" term was one of the terms with a lower p-value and a large gap at the tail end of its data. By focusing in on the variance above 4800, it hopefully makes the value better at forecasting participation chances.

Age showed a similar variance in patterns across space, so I categorized it based on the areas with the largest or smallest gaps between yes and no in the continuous variable plots.

Employed numbers, grouping students, unemployed citizens, and retirees in a group, seemed to create a useful difference between categories.

Months was a variable with high significance, per the regression summary, but the many categories left many individual variables with similar high p-values and low significance. Grouping these months by their relative activity and high ratio of yes's to no's focused the variable into more relevant terms or "high", "med", and "low" collection volume and relative performance.

```{r Feature Engineering, warning = FALSE, message = FALSE}
#Engineer new features
##Summarize job as currently working vs not working. 
subsidyRevised <- 
  subsidy%>%
  mutate(Repairs_4800up = spent_on_repairs-4800,
         Employed = ifelse(job == "unknown", "unknown", 
                    ifelse (job == "student" | 
                      job == "retired" | job == "unemployed", "no", "yes")),
         Age_Blocks = ifelse(age>60 | age == 60, "60+", 
                             ifelse(age < 60 & age > 50, "50-60", 
                             ifelse(age < 51 & age > 29, "30-50",
                                    ifelse(age < 30, "0-30", "unknown")))),
         ActiveMonths = ifelse(month == "mar", "high", 
                      ifelse(month == "nov" | month == "jun" | month == "may", "med", "low")))



#Show DV for these features
subsidyRevised %>%
    dplyr::select(y, Employed, Age_Blocks, ActiveMonths) %>%
    gather(Variable, value, -y) %>%
    count(Variable, value, y)%>%
  mutate(rate = lag(n)/n)%>%
    filter(y=="no")%>%
      ggplot(., aes(value, rate)) +   
        geom_bar(position = "dodge", stat="identity") +
        facet_wrap(~Variable, scales="free") +
        labs(x="Category", y="Percentage of 'Yes' Results",
             title = "Modified Features",
             subtitle = "Categorical features") +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

## Regression Summary

Using these new variables and selecting from a group of those with low p-values and high significance, we split a new data partition and train a revised model. See below that these values have much more focused significance.

```{r Revised Model, warning = FALSE, message = FALSE}

set.seed(999)
trainIndexRevised <- createDataPartition(subsidyRevised$y, p = .65,
                                  list = FALSE,
                                  times = 1)
subsidyTrainRev <- subsidyRevised[ trainIndex,]
subsidyTestRev  <- subsidyRevised[-trainIndex,]

#Make new regression for engineered model
subsidyModelRev <- glm(y_numeric ~ .,
                  data=subsidyTrainRev %>% 
                    dplyr::select(y_numeric, Repairs_4800up, campaign, ActiveMonths, poutcome, contact, cons.price.idx, cons.conf.idx,unemploy_rate, Age_Blocks, mortgage),
                  family="binomial" (link="logit"))

#Compare new and old models
summary(subsidyModelRev)

```

However, it proved difficult to increase the McFadden R-squared metric. This may be due to blind spots in the data or noise that I haven't removed. No other variables appeared to substantailly increase this value on their own, so I chose to leave this metric and move forward with the forecast knowing that this revised model had similar performance with a more efficient set of variables.

```{r McFadden R-Squared Comparison, warning = FALSE, message = FALSE}

tidy(pR2(subsidyModelRev))%>%
mutate(Regression = "Engineered")%>%
rbind(regSubsidySummary)%>%
filter(names == "McFadden")%>%
dplyr::select(-names)%>%
rename(McFadden_R2 = x)%>%
  kable(
  caption = "<strong></strong>",
  escape= FALSE,
  format="html",
  row.names = FALSE,
  align="l")%>%
kable_styling()
```

## CV + Comparison

These cross-validation results below illustrate the same inability to replicate a goodness of fit while using less features, even if those features are more consistently significant. The sensitivity, or ratio of true positive values to all positive values, is quite similar in results between these two models. The revised model has a worse fit of results to the ROC metric, with more of the predictions skewing toward folks taking credit than the actual fit. With a better model, the predictions would have a normal distribution around the fit line.

```{r Kitchen Sink Model CV, warning = FALSE, message = FALSE}

#ROC, Sensitivity, Specificity
ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

cvFit <- train(y ~ .,
                  data=subsidy %>% 
                    dplyr::select(-y_numeric), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl)

cvFit


#Goodness of Fit Metrics
dplyr::select(cvFit$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics, Kitchen Sink Model",
         subtitle = "Across-fold mean reprented as dotted lines")

```

```{r Revised Cross Validation, warning = FALSE, message = FALSE}

#ROC, Sensitivity, Specificity
cvFitRevised <- train(y ~ .,
                  data=subsidyRevised %>% 
                    dplyr::select(y, Repairs_4800up, unemploy_rate, campaign, ActiveMonths, poutcome, contact, cons.price.idx, cons.conf.idx, Age_Blocks, mortgage), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl)

cvFitRevised


#Goodness of Fit Metrics
dplyr::select(cvFitRevised$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#FF006A") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#981FAC", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics, Engineered Model",
         subtitle = "Across-fold mean reprented as dotted lines")

```

# Usefulness of New Model (ROC)

With this model, the rate at which positives (citizens participating) are predicted can be adjusted from 0 to 1. Increasing this threshold will decreases true positives: citizens who would participate to actually get marketing materials. At the same time, increasing the threshold above a certain point will decrease the false positives: spending marketing materials only for the citizen to not participate.

The quality of this curve can be measured through the area under the curve, which is 0.8121.

```{r ROC Metrics, warning = FALSE, message = FALSE}

testProbs <- data.frame(Outcome = as.factor(subsidyTestRev$y_numeric),
                        Probs = predict(subsidyModelRev, subsidyTestRev, type= "response"))

testProbs <- 
  testProbs %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.5 , 1, 0)))

caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")

auc(testProbs$Outcome, testProbs$Probs)

```

The optimal threshold seems to be around 0.60, after which increasing the true positive rate starts to invite more false positives. For baseline analysis, though, we'll use 0.5 as a comparable threshold, then compare to this optimal rate once actual program costs are factored in.

```{r ROC Curve, warning = FALSE, message = FALSE}
ggplot(testProbs, aes(d = as.numeric(Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve - Revised Subsidy Model")
```

# Cost Benefit Analysis

In order to find the right threshold for this campaign, I assign an equation to each confusion metric, or potential outcome of the campaign for each citizen contacted. There are a couple given numbers and some assumed numbers in this section to illustrate how a campaign can be measured as successful.

Let's assume that Emil City has higher property tax rates than Philadelphia, at 2% annually. Those who receive the \$5000 subsidy usually see a \$10,000 benefit to their home value along with an average \$56,000 benefit to the surrounding homes, which the city captures in property tax over a long period of time. That's 0.02 tax \* (10,000 + 56,000) benefits = \$1,320 annually from a one-time subsidy of \$5000. Repairs and appliances last on average [10-15 years](https://www.landmarkhw.com/resources/home-repairs-maintenance/how-long-do-your-homes-systems-and-appliances-last/6/76), so with taxes, the subsidy and campaign cost pay for themselves over time.

Additionally, let's assume any sale of the of the properties would generate income from the real estate transfer tax of 5% (Philadelphia's is about 3.3%). Say that 10% the existing homes are sold annually (high quality leading to a higher sale rate than [the national average](https://www.statista.com/statistics/226144/us-existing-home-sales/)), then we gain an additional 0.10 homes sold \* 0.05 tax \* (10,000+56,000) benefits = about \$200 annually along with the \$1320 property tax.

The program has other direct benefits that are harder to calculate. Similar programs like the Whole-Home Repairs Program mentioned before are said to repay itself by "stabilizing communities" and creating an "early intervention in a cycle which leads to disrepair of homes, to abandonment, to displacement". That noted, those metrics require more data to capture, so these two tax metrics will help us create a threshold where cost/benefit can be maximized. We move forward with an understanding that even if the benefit is still negative, it is not a permanent cost to the city.

## Equations

**True Positive Revenue:** "We predicted the citizen would participate in the subsidy program once marketed to (\$2850 cost), and they did. 25% of participants complete program process and receive the subsidy of \$5000. They and their neighbors benefit from increased housing value (\$10,000 and \$56,000 respectively), and the city captures that annually in property taxes (1.4%) and sales taxes (3.3%). We assume 10% of these homes are sold annually. Repairs are estimated to last 10+ years on average, so annual benefit is applied for 15 years."

Cost/Benefit: -\$2850 marketing + 0.25 successes \* (-\$5000 subsidy + ((\$10,000 + \$56,000)\*0.02 annual property tax + ((\$10,000 + \$56,000)\*0.10 home sales \*0.05 real estate transfer tax))\* 15 years).

**True Negative Revenue:** "We predicted citizen would not participate in the credit program, did not allocate marketing resources toward them. Citizen did not participate in the program, no credit allocated."

Cost/Benefit: \$0

**False Positive Revenue:** "We predicted citizen would participate in credit program, allocated marketing resources toward them (\$2850 cost). citizen did not participate the credit program."

Cost/Benefit: -\$2850

**False Negative Revenue:** "Predicted citizen would not participate in the credit program, did not allocate marketing resources. Citizen participated in the program no cost to the campaign."

Cost/Benefit: \$0

## Table of Costs and Benefits

These are the metrics when assuming a threshold of 0.50.

```{r Cost Benefit Table, warning = FALSE, message = FALSE}

cost_benefit_table <-
   testProbs %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
       gather(Variable, Count) %>%
       mutate(Revenue =
               ifelse(Variable == "True_Negative", Count * 0,
               ifelse(Variable == "True_Positive",(
                 (-2850 + #marketing cost
                  0.25*(-5000 #successful participation and subsidy 
                 +((10000+56000)*0.02 #property tax
                 +(10000+56000)*0.10*0.05) #real estate transfer tax
                 *15)) #applied over 15 years
                 * Count),
               ifelse(Variable == "False_Negative", Count * 0,
               ifelse(Variable == "False_Positive", (-2850) * Count, 0))))) %>%
    bind_cols(data.frame(Description = c(
              "We correctly predicted no participation",
              "We correctly predicted participation, with 25% receiving subsidy",
              "We predicted no particiption and the citizen pariticpated",
              "We predicted particiption and citizen did not participate")))

kable(cost_benefit_table,
       caption = "Cost/Benefit Table") %>% kable_styling()


```

## Confusion Metrics For Each Outcome

Here are two charts that plot the cost/benefit for each outcome (defined above), which can help us see how the costs/benefit equations are performing across the spectrum of thresholds.

```{r Thresholds for Each Confusion Metric, warning = FALSE, message = FALSE}

iterateThresholds <- function(data) {
  x = .01
  all_prediction <- data.frame()
  while (x <= 1) {
  
  this_prediction <-
      testProbs %>%
      mutate(predOutcome = ifelse(Probs > x, 1, 0)) %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
     gather(Variable, Count) %>%
     mutate(Revenue =
               ifelse(Variable == "True_Negative", Count * 0,
               ifelse(Variable == "True_Positive",((-2850  
                                                  + 0.25*(-5000 
                                                  + ((10000+56000)*0.02 
                                                  + (10000+56000)*0.10*0.05)
                                                  *15))
                                                  * Count),
               ifelse(Variable == "False_Negative", (0) * Count,
               ifelse(Variable == "False_Positive", (-2850) * Count, 0)))),
            Threshold = x)
  
  all_prediction <- rbind(all_prediction, this_prediction)
  x <- x + .01
  }
return(all_prediction)
}

whichThreshold <- iterateThresholds(testProbs2)

#Revenue Confusion Matrix Plot
revThresholdPlot <- whichThreshold %>%
  ggplot(.,aes(Threshold, Revenue, colour = Variable)) +
  geom_point() +
  scale_colour_manual(values = palette5[c(5, 1:3)]) +    
  labs(title = "Threshold as a Function of Revenue") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix"))

#Count of Subsidies Confusion Matrix Plot
countThresholdPlot <- whichThreshold %>%
  ggplot(.,aes(Threshold, Count, colour = Variable)) +
  geom_point() +
  scale_colour_manual(values = palette5[c(5, 1:3)]) +    
  labs(title = "Threshold as a function of Total Counts") +
  plotTheme() +
  guides(colour=guide_legend(title = "Confusion Matrix"))

grid.arrange(revThresholdPlot, countThresholdPlot, nrow=2)

```

## Total Revenue and Count of Credits

The two plots below help us determine which threshold maximizes the city's revenue and which maximizes the total amount of citizens receiving the credit. We start out with many false positives, which are a drain on the program's resources, without the true positives to make up for them. As the model allows less true positives, our revenue benefits and the amount of citizens participating in the program increases, and eventually true positives outweigh false positives (seen above). However, after 0.63, the true positives decrease to a similar level of the false positives (near 0) so not many people receive the subsidy and not many who wouldn't are marketed to.

This metric gives us our optimal threshold of 0.63.

```{r Optimal Thresholds , warning = FALSE, message = FALSE}

whichThreshold_revenue <- 
whichThreshold %>% 
    group_by(Threshold) %>% 
    summarize(Revenue = sum(Revenue))%>%
    round(digits=2)

revOptimalPlot <-
  ggplot(whichThreshold_revenue)+
geom_line(aes(x = Threshold, y = Revenue))+
geom_vline(xintercept =  pull(arrange(whichThreshold_revenue, -Revenue)[1,1]))+
  labs(title = "Net Revenue by Threshold",
       subtitle = "Vertical Line Denotes Optimal Threshold")

whichThreshold_counts <- 
    whichThreshold%>% 
    group_by(Threshold)%>%
    mutate(TotalCount = ifelse(Variable == "True_Negative", 0,
                     ifelse(Variable == "True_Positive", Count,
               ifelse(Variable == "False_Negative", 0,
               ifelse(Variable == "False_Positive", Count * -1, 0)))))%>%
    summarize(TotalCount = sum(TotalCount))%>%
  round(digits=2)
  
Optimal_Threshold <- pull(arrange(whichThreshold_counts, -TotalCount)[1,1])

countOptimalPlot <-
    ggplot(whichThreshold_counts)+
geom_line(aes(x = Threshold, y = TotalCount))+
geom_vline(xintercept =  pull(arrange(whichThreshold_counts, -TotalCount)[1,1]))+
  labs(title = "Net Participants by Threshold",
       subtitle = "Vertical Line Denotes Optimal Threshold",
       y= "Total Count (TP - FP)")

grid.arrange(countOptimalPlot, revOptimalPlot, ncol=2)


```

## Optimal Thresholds

See here how 5 more people participate in the program when we fine-tune the model's threshold from 0.5 to 0.63.

```{r 0.50 and Optimal Threshold Measures, warning = FALSE, message = FALSE}

whichThreshold_Summary <-
  whichThreshold_counts %>%
  left_join(whichThreshold_revenue, by="Threshold")%>%
  filter(Threshold == 0.50 | Threshold == Optimal_Threshold)

whichThreshold_Summary%>%
  kable(
    caption = "<strong></strong>",
    escape= FALSE,
    format="html",
    row.names = FALSE,
    align="l")%>%
  kable_styling()

```

# Conclusion

According to this model. Emil City will have to weigh whether it wants to spend drastically more to reach the maximum amount of citizens who would actually participate in the program. Perhaps our model is under-predicting true positives, as evidenced by the ROC curve's skew noted earlier. This would be the best case scenario for the city, as more true-positives means much more revenue and social benefit.

This model does the best it can with available data and time, and successfully optimizes its threshold to give the city a long-term benefit. That said, not many citizens are reached by this campaign and actually participate in the program if this model is put into practice. The difficulty of finding good features to include made the revised model less accurate, according to our own cross-validation metrics. Better insight into the data's categories would help make a better model, as the feature engineering produced sub-par results. To improve the model, I may look into the data more thoroughly to find trends between categories, incorporate other economy data in the vein of unemployment rates, or see if there is a spatial relationship that our current data set does not feature. There could be plenty of opportunity to make this campaign result in a better response rate, but I'd need further insight into what makes Emil City residents attracted a win-win program.
