---
title: "How the Internet Talks"
author: "Eugene Brusilovskiy"
date: "Last updated on 12/2/2022"
output: rmdformats::readthedown
---

## Introduction

The goal of this R Markdown is to use R to obtain Twitter data and to use some text mining packages and mapping packages to examine the content, and if possible, map these tweets.

Here, we will use the ` rtweet` package for obtaining data from Twitter. A great illustration of this package is available here: https://docs.ropensci.org/rtweet/articles/rtweet.html. 

One of the benefits of this package is that unlike other similar packages (e.g., ` twitteR`), ` rtweet` doesn't require us to make a developer account, obtain the API and other tokens, and install many other packages for it to work. All we need to use it is a Twitter account, and we are good to go. 

Nonetheless, ` rtweet` enables us to obtain the same data, which includes:

* Searching Tweets that contain a certain term or hashtag, with output that includes:
     + The tweet date and time, 
     + The text of the tweet, 
     + Whether the tweet was retweeted or favorited, 
     + The number of retweets/replies, 
     + The latitude/longitude or place that the tweet was coming 
     + The language of the tweet, and
     + An automatic designation of whether the tweet was possibly sensitive
* Searching Tweets posted by a specified account,
* Looking up the the followers an account has,
* And many other cool things.

The number of Tweets that we can get is set by Twitter, changes on a regular basis, and will be discussed later.

## Setting Up ` rtweet`

Prior to running anything, we will set the working directory below. You may change it to where the files are stored on your computer.

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:\\MUSAFall\\MUSA500\\HW 6")
```

Now, let's go ahead and install the ` rtweet` package:

```{r warning=FALSE, message=FALSE, cache=FALSE}
options(scipen=999)

#install.packages("rtweet")
library(rtweet)
library(purrr)
```

We then ask ` rtweet` to authenticate us, using the command below:

```{r warning=FALSE, message=FALSE, cache=FALSE}
auth_setup_default()
```

When we run the code above, we get taken to a page that asks us whether we would like to allow RStats to help with the authentication process, create the necessary token, and save it in a directory for future use. We give it the requested permission, and if we aren't already logged into our Twitter account, provide our user name and password. 

In future R sessions, we can still run this command, or alternatively, we can use the ` auth_as("default")` command to tell R that we have already authenticated.

We can then look up how many tweets we can get in one search, how many followers of an account we can get in one search, etc. Once again, this rate changes all the time. For example, the number of tweets containing a search term that you could get in 2021 was in the thousands; currently, it is limited to 900 tweets per 15 minutes. 

For our intents and purposes, this means we should not request more than 900 tweets per query. However, there is a ` retryonratelimit` option in the different ` rtweet` commands that asks the command to wait until there are more calls available and to resume the query.

```{r warning=FALSE, message=FALSE, cache=FALSE}
rate_limit()
```

This information can be seen in the second line of the tibble that's output above.

` ##    resource                 limit remaining reset_at            reset  `
` ##  2 /lists/:id/tweets&GET      900       900 2022-12-02 13:26:29 15 mins`

## Some ` rtweet` Examples

We can also search the tweets that have a certain search term in them. We will use the term "SEPTA" for this example. We will not include retweets here (` include_rts = FALSE`), and we will limit this to 900 tweets, which is the maximum that we can ask for given the current rate limits. Note that if we set ` retryonratelimit = TRUE`, we can get more than 900 tweets, but it will take a long time. 

The ` colnames(SEPTA)` command shows what information is available for each tweet. 

```{r warning=FALSE, message=FALSE, cache=FALSE}
bikeLanes <- search_tweets("bike lane", n = 9000, retryonratelimit = TRUE, include_rts = FALSE)


bikeLanes.geo <- filter(bikeLanes, geo != "NA")

bikeLanes.geoTrue <- 
colnames(bikeLanes)

```

Let's look at the text of the first 10 tweets:

```{r warning=FALSE, message=FALSE, cache=FALSE}
head(bikeLanes$text, n=10)
```

Now, let's look to see whether there's any geographic information associated with the first tweet. Most tweets don't have the place, or latitude and longitude associated with them, because the user needs to manually override the default Twitter settings where the location is not shared.

```{r warning=FALSE, message=FALSE, cache=FALSE}
bikeLanes$place[[1]]$geo
bikeLanes$place[[1]]$coordinates
bikeLanes$place[[1]]$place
```

However, some users manually override these privacy settings and make their location public, meaning that we can use ` rtweet` to stream all geo-enabled Tweets from Philadelphia for 15 seconds. This will require a Google Maps API, which you can easily set up. (Figuring out how to get the API code is left as a very simple exercise for the student. Hint: Google it!)

The stream of these geo-enabled tweets is saved as ` stream_philly`, and we can look at their locations to confirm that they're indeed from Philadlephia.

```{r warning=FALSE, message=FALSE, cache=FALSE}
stream_philly <- tempfile(fileext = ".json")
stream_philly <- stream_tweets(lookup_coords("philadelphia, usa"), timeout = 15, 
                               file_name = stream_philly)
```

We can certainly map the tweets that have geographic coordinates, but this seems to be a futile task, because a very small percentage of tweets have geographic coordinates. 

**However, students may map the tweets for extra credit and present the code**. (To do this, they would want to extract the coordinates of each tweet. For instance, if they want to get the coordinates of the first tweet in the ` stream_philly` list, they can access it by using the command below, and then write a loop to do this for all the tweets, saving the coordinates of all the tweets in a data frame.)

```{r warning=FALSE, message=FALSE, cache=FALSE}
#First let's get the information about the place -- we should see that it's Phila!
stream_philly$place[[1]]$place
#Note that just because the place is not missing, it doesn't mean that the latitude and longitude aren't missing. 
stream_philly$place[[1]]$coordinates
```



## Text Mining: Data Cleaning

First, let's load the required packages. 

```{r warning=FALSE, message=FALSE, cache=FALSE}
#install.packages(c("tm","RTextTools","SnowballC"))

library(tm)
library(RTextTools)
library(SnowballC)
```

Now, let's save the twitter text as a data frame:

```{r warning=FALSE, message=FALSE, cache=FALSE}
bikeLanes_Text <- data.frame(text = bikeLanes$text)
```

After this, we can convert the data frame to a corpus. Corpora (plural of corpus) are collections of documents containing (natural language) text. They are used in packages which employ the infrastructure provided by the text mining package ` tm`. Specifically, each tweet is saved as a separate document

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- Corpus(VectorSource(bikeLanes_Text$text))
myCorpus

#added the following: 

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "&amp;")

myCorpus <- tm_map(myCorpus, function(x) iconv(x, "latin1", "ASCII", sub=""))

myCorpus <- tm_map(myCorpus, removeNumbers)

myCorpus <- tm_map(myCorpus, removePunctuation)

stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

myCorpus <- tm_map(myCorpus, removeWords, c("By", "the", "http", "may", "ttp",
                                           "qhb...", "https", "http...", "for", 
                                           "tco", "the", "that", "this", "the", "that", "this", "you", "amp", "ive", "gtfnaxk", "just", "they", "that", "someone", "its","december"))

myCorpus <- tm_map(myCorpus, stripWhitespace)


dtm <- DocumentTermMatrix(myCorpus)
dtm
dim(dtm)

freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, n=100)

palette_9_colors <- c("#ffffe5", "#f7fcb9","#d9f0a3","#addd8e","#78c679","#41ab5d","#238443","#006837","#004529")
palette_5_colors <- c("#78c679","#41ab5d","#238443","#006837","#004529")



wordcloud(names(freq), freq, min.freq=50, colors=palette_5_colors)

```

Now that we have the data in a corpus, let's do some data cleaning, by converting a bunch of special characters (e.g., @, /, ], $) to a space. Here, we can ignore the "transformation drops documents" warning messages if they come up.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#     Defining the toSpace function
#     Removing special characters


```

We can again look at the first entry of the corpus:

```{r warning=FALSE, message=FALSE, cache=FALSE}
strwrap(myCorpus[[2]])
```
Now, let's remove non-ASCII characters from the tweets and look at the first entry of the corpus again:

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
strwrap(myCorpus[[1]])
```

Now, let's remove numbers and look at the first entry of the corpus again:

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, removeNumbers)
strwrap(myCorpus[[1]])
```

Now, let's remove punctuation and look at the first entry of the corpus again:

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, removePunctuation)
strwrap(myCorpus[[1]])
```
Now, let's see if we can get a list of English stop words (e.g., a, to) that we can remove from the tweets. We will do that and look at the first entry of the corpus again.

```{r warning=FALSE, message=FALSE, cache=FALSE}
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
strwrap(myCorpus[[1]])
```

We can also remove additional stop words, such as "http", and look at the first entry of the corpus again.

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, removeWords,c("By", "the", "http", "may", "ttp",
                                           "qhb...", "https", "http...", "for", 
                                           "tco", "the", "that"))
strwrap(myCorpus[[1]])
```

We can also remove whitespace characters, which do not correspond to a visible mark, but typically do occupy an area on a page. Again, we can look at the first entry in the corpus after we do that.

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, stripWhitespace)
strwrap(myCorpus[[1]])
```
Lastly, we can play around with stemming. This removes common word suffixes and endings like "es", "ed", "ing", etc. Sometimes we might not want that. In previous years, we looked at tweets containing the term "winteriscoming", but with stemming, this became "winteriscom". Again, we can look at the first entry of the corpus.

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm_map(myCorpus, stemDocument)
strwrap(myCorpus[[1]])
```

## Text Mining: Creating a Document Term Matrix and Word Cloud

A Document Term Matrix (DTM) is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a DTM, rows correspond to documents (tweets) in the collection and columns correspond to terms. Source: https://en.wikipedia.org/wiki/Document-term_matrix

For example, see: http://d3j5m2z8950src.cloudfront.net/sites/default/files/TM16.jpg

```{r warning=FALSE, message=FALSE, cache=FALSE}
dtm <- DocumentTermMatrix(myCorpus)
dtm
dim(dtm)
```
Now, let's see what the 50 most frequent terms that appear in the document term matrix (i.e., the 900 tweets we are using here) are, and the frequency of their appearance:

```{r warning=FALSE, message=FALSE, cache=FALSE}
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, n=50)
```

Now, let's create a word cloud with some of these frequent terms. Here, we specify that in order to be included in the word cloud, the term needs to appear at least 30 times in our document term matrix. In the resulting word cloud, words that appear more frequently are larger than those that appear less frequently. This is a good way to summarize what people are discussing in their tweets.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#install.packages("wordcloud")
library(wordcloud)

wordcloud(names(freq), freq, min.freq=20, colors=brewer.pal(6, "Dark2"))
```


```{r warning=FALSE, message=FALSE, cache=FALSE}
```
